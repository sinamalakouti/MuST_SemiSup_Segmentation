#trianing and test splits
train_mode: "all_train2018_sup"
val_mode: "test2019_new"
test_mode: "only_test2018"
train_unsup_mode: "train2018_semi_unsup"
train_sup_mode: "train2018_semi_sup"
train_sup_rate: 10 #rate of labeled data
#data loader and pre-processing
t1: True
t2: True
t1ce: True
augment: True
wmh_threshold: 0.5
mixup_threshold: None
intensity_rescale: True
#model
model: "PGS" #PGS,PGSMT, UNET
semi_mode: "shared"   # shared, MT
gradient_stop_strategy: "stop_feature" #stop_feature, stop_output
information_passing_strategy: "teacher" #student, teacher
oneHot: False
#optimizer and scheduler
scheduler_step_size: 5
lr: 0.001 #learning rate
lr_gamma: 0.3
optimizer: "SGD" #SGD OR ADAM
#losses
sup_loss: 'CE' # CE, DSC, FOCAL, CE_DSC, CE_FOCAL, DSC_FOCAL, CE_FOCAL_DSC
consistency_loss: 'CE' # CE, KL, DSC
consist_w_unsup:
  ramp_up: 'linear_rampup' #ramp_types = ['sigmoid_rampup', 'linear_rampup', 'cosine_rampup', 'log_rampup', 'exp_rampup']
  final_w: 5
  rampup_ends: 4
#hyper parameters
experiment_mode: "semi"
n_epochs: 50
batch_size: 32
parallel: True
cuda: "0,1"
seed: 41
information: "no input augemtation for unsup_loader, semi supervised, pass teacher's features,
shared, supervised: student, stop gradietn: teach feature, weight_unsup = linear_ramp(final_w = 2, step =4)"



