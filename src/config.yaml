
# THIS IS FOR UNET MODEL
#previous configs:
#lr: 0.001
#scheduler_step_size: 3
#lr_gamma: 0.1


#trianing and test splits
train_mode: "all_train2018_sup"
val_mode: "test2019_new"
test_mode: "test2020_new"
train_unsup_mode: "train2018_semi_unsup"
train_sup_mode: "train2018_semi_sup"
train_sup_rate: 10 #rate of labeled data
#data loader and pre-processing
t1: True
t2: True
t1ce: True
augment: True
wmh_threshold: 0.5
mixup_threshold: None
intensity_rescale: True
#model
model: "UNET" #PGS,PGSMT, UNET
semi_mode: "shared"   # shared, MT
gradient_stop_strategy: "stop_feature" #stop_feature, stop_output
information_passing_strategy: "teacher" #student, teacher
oneHot: False
num_perturbators: 4
#optimizer
optimizer: "SGD" #SGD OR ADAM
#supervised training setting
supervised_training:
  sup_loss: 'CE' # CE, DSC, FOCAL, CE_DSC, CE_FOCAL, DSC_FOCAL, CE_FOCAL_DSC
  lr: 0.01
  lr_gamma: 0.5
  scheduler_step_size: 5
#unsupervised training
unsupervised_training:
  consistency_loss: 'MSE' # CE, KL, MSE, DSC
  lr: 0.001
  lr_gamma: 0.5
  scheduler_step_size: 10
  consist_w_unsup:
    rampup: 'linear_rampup' #ramp_types = ['sigmoid_rampup', 'linear_rampup', 'cosine_rampup', 'log_rampup', 'exp_rampup']
    final_w: 10
    rampup_ends: 20

#hyper parameters
experiment_mode: "semi_alternate"
n_epochs: 70
batch_size: 32
parallel: True
cuda: "0,1"
seed: 41
information: " trainPgs_sup_upSample, no input augemtation for unsup_loader, semi supervised, pass teacher's features,
shared, supervised: student, stop gradietn: teach feature, weight_unsup = linear_ramp(final_w = 5, step =4)"



