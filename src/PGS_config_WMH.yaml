#trianing and test splits
train_sup_rate: 5 #rate of labeled data
#data loader and pre-processing
t1: True
wmh_threshold: 0.5
mixup_threshold: None
intensity_rescale: True
#model
model: "PGS" #PGS,PGSMT, UNET
semi_mode: "shared"   # shared, MT
gradient_stop_strategy: "stop_feature" #stop_feature, stop_output
information_passing_strategy: "teacher" #student, teacher
oneHot: False
num_perturbators: 4
#optimizer
optimizer: "SGD" #SGD OR ADAM
#supervised training setting
supervised_training:
  sup_loss: 'CE' # CE, DSC, FOCAL, CE_DSC, CE_FOCAL, DSC_FOCAL, CE_FOCAL_DSC
  lr: 0.01
  lr_gamma: 0.5
  scheduler_step_size: 20
#unsupervised training
unsupervised_training:
  consistency_loss: 'MSE' # CE, KL, MSE, DSC
  lr: 0.001
  lr_gamma: 0.5
  scheduler_step_size: 30
  consist_w_unsup:
    rampup: 'linear_rampup' #ramp_types = ['sigmoid_rampup', 'linear_rampup', 'cosine_rampup', 'log_rampup', 'exp_rampup']
    final_w: 10
    rampup_ends: 20
#hyper parameters
experiment_mode: "partially_sup"
n_epochs: 80
batch_size: 16
parallel: False
cuda: "0"
seed: 41
information: " "



